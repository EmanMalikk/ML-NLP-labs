#tokenization with nltk
import nltk
from nltk.tokenize import word_tokenize
file = open()
originalText = file.read()
print(originalText)

tokens = word_tokenize(originalText)
print(tokens)